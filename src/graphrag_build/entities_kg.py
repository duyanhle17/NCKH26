import os, json, re, time, gc, logging
from typing import Any, Dict, List, Optional, Set, Tuple
from collections import defaultdict
from threading import Lock
from concurrent.futures import ThreadPoolExecutor, as_completed

import networkx as nx

from openai import OpenAI

logger = logging.getLogger("KG_LLM")

# =========================
# Adaptive rate limiter
# =========================
class AdaptiveRateLimiter:
    def __init__(self):
        self.lock = Lock()
        self.request_count = 0
        self.start_time = time.time()
        self.consecutive_429 = 0
        self.backoff_until = 0

    def wait(self):
        with self.lock:
            now = time.time()
            if now < self.backoff_until:
                sleep_time = self.backoff_until - now
                logger.info(f"â³ Backoff: sleeping {sleep_time:.1f}s")
                time.sleep(sleep_time)
            self.request_count += 1

    def report_success(self):
        with self.lock:
            self.consecutive_429 = 0

    def report_rate_limit(self):
        with self.lock:
            self.consecutive_429 += 1
            backoff_seconds = min(2 ** self.consecutive_429, 30)
            self.backoff_until = time.time() + backoff_seconds
            logger.warning(f"âš ï¸ Rate limit! Backoff {backoff_seconds}s (attempt {self.consecutive_429})")

    def get_stats(self) -> str:
        elapsed = time.time() - self.start_time
        rpm = self.request_count / (elapsed / 60) if elapsed > 0 else 0
        return f"Requests: {self.request_count} | Elapsed: {elapsed:.1f}s | Rate: {rpm:.1f} req/min"

rate_limiter = AdaptiveRateLimiter()

# =========================
# LLM client (Kimi K2 Instruct via NVIDIA NIM)
# =========================
def make_llm_client() -> OpenAI:
    """
    Táº¡o client káº¿t ná»‘i Kimi K2 Instruct qua NVIDIA NIM API.
    Cáº§n set biáº¿n mÃ´i trÆ°á»ng: NVAPI_KEY

    CÃ¡ch thÃªm API key:
      Terminal (Mac/Linux): export NVAPI_KEY="nvapi-..."
      Hoáº·c thÃªm vÃ o ~/.zshrc / ~/.bashrc Ä‘á»ƒ tá»± Ä‘á»™ng load.
    """
    api_key = os.getenv("NVAPI_KEY")
    if not api_key:
        raise RuntimeError(
            "Missing NVAPI_KEY env var.\n"
            "Set it: export NVAPI_KEY='nvapi-your-key-here'"
        )
    return OpenAI(
        base_url="https://integrate.api.nvidia.com/v1",
        api_key=api_key,
    )

def call_llm(client: OpenAI, model: str, prompt: str, temperature: float = 0.1, max_tokens: int = 2048) -> str:
    """Gá»i LLM vá»›i retry + adaptive rate limiting."""
    for attempt in range(5):
        try:
            rate_limiter.wait()
            resp = client.chat.completions.create(
                model=model,
                messages=[{"role": "user", "content": prompt}],
                temperature=temperature,
                max_tokens=max_tokens,
            )
            rate_limiter.report_success()
            return (resp.choices[0].message.content or "").strip()
        except Exception as e:
            s = str(e).lower()
            if "429" in s or "rate" in s or "too many" in s:
                rate_limiter.report_rate_limit()
            elif "timeout" in s or "connection" in s:
                wait = 2 ** attempt
                logger.warning(f"Connection error (attempt {attempt+1}), retry in {wait}s: {e}")
                time.sleep(wait)
            else:
                logger.warning(f"LLM call failed (attempt {attempt+1}): {e}")
                time.sleep(1)
    logger.error("LLM call failed after 5 attempts, returning empty string.")
    return ""

# =========================
# Prompts (Vietnamese legal document â€” ThÃ´ng tÆ°)
# =========================
ENTITY_EXTRACTION_PROMPT = """Báº¡n lÃ  chuyÃªn gia phÃ¢n tÃ­ch vÄƒn báº£n phÃ¡p luáº­t Viá»‡t Nam, chuyÃªn vá» ThÃ´ng tÆ°, Nghá»‹ Ä‘á»‹nh, Quyáº¿t Ä‘á»‹nh.

HÃ£y trÃ­ch xuáº¥t Táº¤T Cáº¢ entities quan trá»ng tá»« Ä‘oáº¡n vÄƒn báº£n phÃ¡p luáº­t dÆ°á»›i Ä‘Ã¢y.

CÃ¡c loáº¡i entity Cáº¦N trÃ­ch xuáº¥t ({entity_types}):
- CÆ _QUAN_BAN_HÃ€NH: Bá»™, á»¦y ban, cÆ¡ quan nhÃ  nÆ°á»›c ban hÃ nh vÄƒn báº£n (VD: Bá»™ TÃ i chÃ­nh, Bá»™ XÃ¢y dá»±ng)
- VÄ‚N_Báº¢N_PHÃP_LUáº¬T: ThÃ´ng tÆ°, Nghá»‹ Ä‘á»‹nh, PhÃ¡p lá»‡nh, Quyáº¿t Ä‘á»‹nh kÃ¨m sá»‘ hiá»‡u (VD: ThÃ´ng tÆ° sá»‘ 01/1999/TT-BXD)
- LOáº I_THUáº¾_PHÃ: TÃªn loáº¡i thuáº¿ hoáº·c phÃ­, lá»‡ phÃ­ (VD: Thuáº¿ giÃ¡ trá»‹ gia tÄƒng, Lá»‡ phÃ­ cáº¥p phÃ©p, PhÃ­ mÃ´i giá»›i)
- Má»¨C_THU_PHÃ: Má»©c phÃ­ cá»¥ thá»ƒ báº±ng sá»‘, tá»· lá»‡ pháº§n trÄƒm (VD: 0,75% trá»‹ giÃ¡ giao dá»‹ch, 500.000 Ä‘á»“ng)
- Äá»I_TÆ¯á»¢NG_ÃP_Dá»¤NG: Tá»• chá»©c, cÃ¡ nhÃ¢n hoáº·c loáº¡i phÆ°Æ¡ng tiá»‡n chá»‹u sá»± Ä‘iá»u chá»‰nh (VD: doanh nghiá»‡p xÃ¢y dá»±ng, xe quÃ¢n sá»±)
- HOáº T_Äá»˜NG_ÄÆ¯á»¢C_ÄIá»€U_CHá»ˆNH: HÃ nh vi, hoáº¡t Ä‘á»™ng kinh táº¿ Ä‘Æ°á»£c quy Ä‘á»‹nh (VD: mÃ´i giá»›i chá»©ng khoÃ¡n, láº­p dá»± toÃ¡n xÃ¢y láº¯p)
- ÄIá»€U_KHOáº¢N: Äiá»u, khoáº£n, má»¥c, Ä‘iá»ƒm cá»¥ thá»ƒ (VD: Äiá»u 13, Äiá»u 14 Nghá»‹ Ä‘á»‹nh 25/2004/NÄ-CP)
- THá»œI_Háº N_HIá»†U_Lá»°C: NgÃ y báº¯t Ä‘áº§u hoáº·c káº¿t thÃºc hiá»‡u lá»±c (VD: cÃ³ hiá»‡u lá»±c tá»« ngÃ y 01/01/1999)

VÄ‚N Báº¢N:
{chunk_text}

Quy táº¯c:
- TrÃ­ch xuáº¥t name ÄÃšNG nhÆ° trong vÄƒn báº£n, khÃ´ng viáº¿t táº¯t tÃ¹y tiá»‡n
- Vá»›i vÄƒn báº£n phÃ¡p luáº­t: luÃ´n láº¥y Ä‘á»§ sá»‘ hiá»‡u (VD: "Nghá»‹ Ä‘á»‹nh sá»‘ 28/1998/NÄ-CP")
- Vá»›i má»©c phÃ­/thuáº¿: láº¥y cáº£ giÃ¡ trá»‹ vÃ  Ä‘Æ¡n vá»‹ (VD: "0,75% trá»‹ giÃ¡ giao dá»‹ch")
- description: tÃ³m táº¯t ngáº¯n gá»n vai trÃ² hoáº·c ná»™i dung trong ngá»¯ cáº£nh nÃ y

Tráº£ vá» JSON array (khÃ´ng giáº£i thÃ­ch, khÃ´ng markdown):
[
  {{"name": "...", "type": "...", "description": "..."}},
  ...
]
"""

RELATIONSHIP_EXTRACTION_PROMPT = """Báº¡n lÃ  chuyÃªn gia phÃ¢n tÃ­ch vÄƒn báº£n phÃ¡p luáº­t Viá»‡t Nam.

Dá»±a trÃªn Ä‘oáº¡n vÄƒn báº£n vÃ  danh sÃ¡ch entities Ä‘Ã£ trÃ­ch xuáº¥t, hÃ£y xÃ¡c Ä‘á»‹nh cÃ¡c Má»I QUAN Há»† giá»¯a cÃ¡c entities.

ENTITIES:
{entities_json}

VÄ‚N Báº¢N:
{chunk_text}

CÃ¡c loáº¡i relation phÃ¹ há»£p vá»›i vÄƒn báº£n phÃ¡p luáº­t VN:
- ban_hÃ nh_bá»Ÿi       : vÄƒn báº£n Ä‘Æ°á»£c ban hÃ nh bá»Ÿi cÆ¡ quan
- cÄƒn_cá»©             : vÄƒn báº£n/quy Ä‘á»‹nh dá»±a trÃªn vÄƒn báº£n khÃ¡c
- sá»­a_Ä‘á»•i_bá»•_sung    : vÄƒn báº£n sá»­a Ä‘á»•i vÄƒn báº£n cÅ©
- quy_Ä‘á»‹nh_má»©c       : vÄƒn báº£n/loáº¡i phÃ­ cÃ³ má»©c thu cá»¥ thá»ƒ
- Ã¡p_dá»¥ng_cho        : quy Ä‘á»‹nh Ã¡p dá»¥ng cho Ä‘á»‘i tÆ°á»£ng nÃ o
- Ä‘iá»u_chá»‰nh         : vÄƒn báº£n Ä‘iá»u chá»‰nh hoáº¡t Ä‘á»™ng nÃ o
- thu_bá»Ÿi            : loáº¡i phÃ­ Ä‘Æ°á»£c thu bá»Ÿi cÆ¡ quan nÃ o
- ná»™p_vÃ o            : tiá»n phÃ­ ná»™p vÃ o ngÃ¢n sÃ¡ch/tÃ i khoáº£n nÃ o
- cÃ³_hiá»‡u_lá»±c_tá»«     : thá»i Ä‘iá»ƒm vÄƒn báº£n cÃ³ hiá»‡u lá»±c
- thay_tháº¿           : vÄƒn báº£n má»›i thay tháº¿ vÄƒn báº£n cÅ©
- hÆ°á»›ng_dáº«n_thi_hÃ nh : vÄƒn báº£n hÆ°á»›ng dáº«n thi hÃ nh vÄƒn báº£n khÃ¡c
- bao_gá»“m            : Ä‘á»‘i tÆ°á»£ng/ná»™i dung bao gá»“m thÃ nh pháº§n con
- liÃªn_quan          : quan há»‡ chung khi khÃ´ng xÃ¡c Ä‘á»‹nh Ä‘Æ°á»£c loáº¡i cá»¥ thá»ƒ

Quy táº¯c:
- Chá»‰ táº¡o relationship náº¿u cÃ³ báº±ng chá»©ng rÃµ rÃ ng trong vÄƒn báº£n
- source vÃ  target pháº£i KHá»šP CHÃNH XÃC vá»›i name trong danh sÃ¡ch entities
- weight: 0.9=ráº¥t cháº¯c cháº¯n, 0.7=cháº¯c cháº¯n, 0.5=cÃ³ thá»ƒ

Tráº£ vá» JSON array (khÃ´ng giáº£i thÃ­ch, khÃ´ng markdown):
[
  {{"source": "...", "target": "...", "relation": "...", "description": "...", "weight": 0.8}},
  ...
]
"""

# =========================
# Helpers
# =========================
def parse_json_safe(text: str) -> List[Dict[str, Any]]:
    text = (text or "").strip()
    m = re.search(r"\[[\s\S]*\]", text)
    if m:
        text = m.group()
    try:
        arr = json.loads(text)
        return arr if isinstance(arr, list) else []
    except Exception:
        return []

def norm_entity_name(name: str) -> str:
    # Báº¡n Ä‘ang dÃ¹ng UPPER trong V2, giá»¯ nguyÃªn Ä‘á»ƒ merge dá»…
    return (name or "").strip().upper()

def extract_terms_yake(text: str, top_k: int = 10, lan: str = "vi") -> List[str]:
    """
    YAKE há»— trá»£: chá»‰ Ä‘á»ƒ bá»• sung entities bá»‹ sÃ³t.
    - lan: "vi" (VN) hoáº·c "en"
    """
    try:
        import yake
        kw = yake.KeywordExtractor(lan=lan, n=3, top=top_k, dedupLim=0.9, windowsSize=1)
        kws = kw.extract_keywords(text)
        kws = sorted(kws, key=lambda x: x[1])[:top_k]
        out = []
        for k, _ in kws:
            k = (k or "").strip()
            if len(k) >= 3:
                out.append(k)
        return out
    except Exception:
        return []

# =========================
# Chunk processing
# =========================
def process_single_chunk(
    client: OpenAI,
    llm_model: str,
    chunk: str,
    chunk_idx: int,
    entity_types: List[str],
    yake_top_k: int,
    yake_lang: str,
    max_text_chars: int = 3500,
) -> Tuple[int, List[Dict[str, Any]], List[Dict[str, Any]]]:

    chunk_short = chunk[:max_text_chars]

    # ---- Entities via LLM
    prompt_ent = ENTITY_EXTRACTION_PROMPT.format(
        entity_types=", ".join(entity_types),
        chunk_text=chunk_short
    )
    ent_text = call_llm(client, llm_model, prompt_ent, temperature=0.1, max_tokens=2000)
    ents_raw = parse_json_safe(ent_text)

    entities: List[Dict[str, Any]] = []
    for e in ents_raw:
        if not isinstance(e, dict):
            continue
        name = norm_entity_name(e.get("name", ""))
        if not name:
            continue
        entities.append({
            "name": name,
            "type": e.get("type", "UNKNOWN"),
            "description": (e.get("description", "") or "").strip(),
            "source_chunk": chunk_idx,
        })

    # ---- YAKE supplement (optional)
    # Chá»‰ thÃªm náº¿u LLM ra Ã­t entities hoáº·c Ä‘á»ƒ tÄƒng recall
    yake_terms = extract_terms_yake(chunk_short, top_k=yake_top_k, lan=yake_lang) if yake_top_k > 0 else []
    existing = {e["name"] for e in entities}
    for t in yake_terms:
        n = norm_entity_name(t)
        if n and n not in existing:
            entities.append({
                "name": n,
                "type": "TERM_YAKE",
                "description": "Keyword extracted by YAKE",
                "source_chunk": chunk_idx,
            })
            existing.add(n)

    # ---- Relationships via LLM
    relationships: List[Dict[str, Any]] = []
    if entities:
        entities_json = json.dumps(
            [{"name": e["name"], "type": e["type"]} for e in entities],
            ensure_ascii=False,
            indent=2
        )
        prompt_rel = RELATIONSHIP_EXTRACTION_PROMPT.format(
            entities_json=entities_json,
            chunk_text=chunk_short
        )
        rel_text = call_llm(client, llm_model, prompt_rel, temperature=0.1, max_tokens=2000)
        rels_raw = parse_json_safe(rel_text)

        names = {e["name"] for e in entities}
        for r in rels_raw:
            if not isinstance(r, dict):
                continue
            src = norm_entity_name(r.get("source", ""))
            tgt = norm_entity_name(r.get("target", ""))
            if not src or not tgt or src == tgt:
                continue
            if src not in names or tgt not in names:
                # chá»‰ giá»¯ rel mÃ  node cÃ³ trong chunk list
                continue
            try:
                w = float(r.get("weight", 0.5))
            except Exception:
                w = 0.5
            relationships.append({
                "source": src,
                "target": tgt,
                "relation": (r.get("relation", "liÃªn_quan") or "liÃªn_quan").strip(),
                "description": (r.get("description", "") or "").strip(),
                "weight": max(0.1, min(1.0, w)),
                "source_chunk": chunk_idx,
            })

    return chunk_idx, entities, relationships

# =========================
# Build KG (parallel + checkpoint)
# =========================
def build_kg_llm(
    dataset: List[str],
    llm_model: str,
    entity_types: List[str],
    max_workers: int = 12,
    batch_size: int = 200,
    checkpoint_dir: Optional[str] = None,
    yake_top_k: int = 8,
    yake_lang: str = "vi",
) -> Tuple[nx.DiGraph, List[Set[str]], Dict[str, Set[int]], List[Dict[str, Any]], List[Dict[str, Any]]]:

    if checkpoint_dir:
        os.makedirs(checkpoint_dir, exist_ok=True)
        progress_file = os.path.join(checkpoint_dir, "progress.json")
    else:
        progress_file = None

    client = make_llm_client()

    total = len(dataset)
    num_batches = (total + batch_size - 1) // batch_size

    # resume
    start_batch = 0
    if progress_file and os.path.exists(progress_file):
        with open(progress_file, "r", encoding="utf-8") as f:
            prog = json.load(f)
            start_batch = int(prog.get("completed_batches", 0))

    logger.info(f"ðŸš€ LLM KG build | chunks={total} | batches={num_batches} | workers={max_workers} | resume_batch={start_batch}")

    all_entities: List[Dict[str, Any]] = []
    all_relationships: List[Dict[str, Any]] = []
    chunk_entities_map: List[List[Dict[str, Any]]] = [ [] for _ in range(total) ]

    for b in range(start_batch, num_batches):
        bs = b * batch_size
        be = min(bs + batch_size, total)
        logger.info(f"ðŸ“¦ Batch {b+1}/{num_batches} chunks {bs}-{be-1}")

        args_list = [(dataset[i], i) for i in range(bs, be)]
        batch_results: Dict[int, Tuple[List[Dict[str, Any]], List[Dict[str, Any]]]] = {}

        t0 = time.time()
        with ThreadPoolExecutor(max_workers=max_workers) as ex:
            futures = {
                ex.submit(
                    process_single_chunk,
                    client,
                    llm_model,
                    chunk,
                    idx,
                    entity_types,
                    yake_top_k,
                    yake_lang,
                ): idx
                for chunk, idx in args_list
            }
            done = 0
            for fut in as_completed(futures):
                idx = futures[fut]
                try:
                    _, ents, rels = fut.result()
                    batch_results[idx] = (ents, rels)
                except Exception as e:
                    logger.error(f"Chunk {idx} failed: {e}")
                    batch_results[idx] = ([], [])
                done += 1
                if done % 10 == 0 or done == (be - bs):
                    elapsed = time.time() - t0
                    rate = done / elapsed if elapsed > 0 else 0
                    logger.info(f"   [{done}/{be-bs}] {rate:.2f} chunks/s | {rate_limiter.get_stats()}")

        # collect + checkpoint
        batch_entities = []
        batch_relationships = []

        for idx in range(bs, be):
            ents, rels = batch_results.get(idx, ([], []))
            batch_entities.extend(ents)
            batch_relationships.extend(rels)
            chunk_entities_map[idx] = ents

        all_entities.extend(batch_entities)
        all_relationships.extend(batch_relationships)

        if checkpoint_dir:
            batch_file = os.path.join(checkpoint_dir, f"batch_{b:04d}.json")
            with open(batch_file, "w", encoding="utf-8") as f:
                json.dump({
                    "batch_idx": b,
                    "start_chunk": bs,
                    "end_chunk": be,
                    "entities": batch_entities,
                    "relationships": batch_relationships,
                }, f, ensure_ascii=False)

            with open(progress_file, "w", encoding="utf-8") as f:
                json.dump({
                    "completed_batches": b + 1,
                    "total_batches": num_batches,
                    "total_entities": len(all_entities),
                    "total_relationships": len(all_relationships),
                }, f, ensure_ascii=False)

        # memory
        del batch_results, batch_entities, batch_relationships
        gc.collect()

    # ========= Merge entities =========
    entity_data = defaultdict(lambda: {"types": [], "descriptions": [], "source_chunks": set()})
    for e in all_entities:
        name = e["name"]
        entity_data[name]["types"].append(e.get("type", "UNKNOWN"))
        if e.get("description"):
            entity_data[name]["descriptions"].append(e["description"])
        entity_data[name]["source_chunks"].add(int(e.get("source_chunk", -1)))

    kg = nx.DiGraph()
    for name, data in entity_data.items():
        # choose most common type
        counts = defaultdict(int)
        for t in data["types"]:
            counts[t] += 1
        main_type = max(counts, key=counts.get) if counts else "UNKNOWN"

        # merge a few unique descriptions
        uniq_desc = []
        for d in data["descriptions"]:
            if d not in uniq_desc:
                uniq_desc.append(d)
            if len(uniq_desc) >= 3:
                break

        kg.add_node(
            name,
            entity_type=main_type,
            description=" | ".join(uniq_desc),
            source_chunks=sorted(list(data["source_chunks"])),
        )

    # ========= Merge relationships =========
    edge_data = defaultdict(lambda: {"relations": [], "descriptions": [], "weights": [], "source_chunks": set()})
    for r in all_relationships:
        key = (r["source"], r["target"], r.get("relation", "liÃªn_quan"))
        edge_data[key]["relations"].append(r.get("relation", "liÃªn_quan"))
        if r.get("description"):
            edge_data[key]["descriptions"].append(r["description"])
        edge_data[key]["weights"].append(float(r.get("weight", 0.5)))
        edge_data[key]["source_chunks"].add(int(r.get("source_chunk", -1)))

    for (src, tgt, rel), data in edge_data.items():
        if not kg.has_node(src) or not kg.has_node(tgt):
            continue

        # pick most frequent rel label
        counts = defaultdict(int)
        for r in data["relations"]:
            counts[r] += 1
        main_rel = max(counts, key=counts.get) if counts else rel

        uniq_desc = []
        for d in data["descriptions"]:
            if d not in uniq_desc:
                uniq_desc.append(d)
            if len(uniq_desc) >= 2:
                break

        avg_weight = sum(data["weights"]) / max(1, len(data["weights"]))

        kg.add_edge(
            src,
            tgt,
            relation=main_rel,
            description=" | ".join(uniq_desc),
            weight=avg_weight,
            source_chunks=sorted(list(data["source_chunks"])),
        )

    # ========= Build chunk_entities + entity_to_chunks =========
    chunk_entities: List[Set[str]] = []
    entity_to_chunks: Dict[str, Set[int]] = defaultdict(set)

    for idx, ents in enumerate(chunk_entities_map):
        names = set(e["name"] for e in ents if e.get("name"))
        chunk_entities.append(names)
        for n in names:
            entity_to_chunks[n].add(idx)

    logger.info(f"âœ… KG built (LLM): nodes={kg.number_of_nodes()} edges={kg.number_of_edges()}")
    return kg, chunk_entities, dict(entity_to_chunks), all_entities, all_relationships