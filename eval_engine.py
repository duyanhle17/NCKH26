import os, json, pickle, time
from typing import List, Dict, Tuple
import numpy as np
import networkx as nx
import faiss
from sentence_transformers import SentenceTransformer
from openai import OpenAI
from tqdm import tqdm

# ============================================================
# LOAD ARTIFACTS
# ============================================================
CACHE_DIR_V2 = "./artifact_faiss"

print("ğŸ”„ Loading GraphRAG artifacts...")

with open(os.path.join(CACHE_DIR_V2, "meta.json"), "r", encoding="utf-8") as f:
    META_V2 = json.load(f)

with open(os.path.join(CACHE_DIR_V2, "chunks.json"), "r", encoding="utf-8") as f:
    CHUNKS_V2 = json.load(f)

with open(os.path.join(CACHE_DIR_V2, "kg.pkl"), "rb") as f:
    KG_V2 = pickle.load(f)

# Build entity to chunks mapping for Hybrid Search
# KG_V2 is a DiGraph where nodes contain 'source_chunks'
ENTITY_TO_CHUNKS = {}
for node, data in KG_V2.nodes(data=True):
    ENTITY_TO_CHUNKS[node] = data.get("source_chunks", [])

CHUNK_INDEX_V2 = faiss.read_index(os.path.join(CACHE_DIR_V2, "faiss_chunks.index"))

entity_index_path = os.path.join(CACHE_DIR_V2, "faiss_entities.index")
if os.path.exists(entity_index_path):
    ENTITY_INDEX_V2 = faiss.read_index(entity_index_path)
    with open(os.path.join(CACHE_DIR_V2, "entities.json"), "r", encoding="utf-8") as f:
        ENTITY_NAMES_V2 = json.load(f)
else:
    ENTITY_INDEX_V2 = None
    ENTITY_NAMES_V2 = []

EMBEDDER_V2 = SentenceTransformer(META_V2["embedding_model"])

print("âœ… Artifacts loaded!")

# ============================================================
# LLM CLIENT
# ============================================================
LLM_MODEL_QUERY = "moonshotai/kimi-k2-instruct"
if not os.getenv("NVAPI_KEY"):
    raise RuntimeError("Missing NVAPI_KEY env var.")

query_llm_client = OpenAI(
    base_url="https://integrate.api.nvidia.com/v1",
    api_key=os.getenv("NVAPI_KEY")
)

def call_llm_query(prompt: str, temperature: float = 0.1, max_tokens: int = 1024) -> str:
    try:
        resp = query_llm_client.chat.completions.create(
            model=LLM_MODEL_QUERY,
            messages=[{"role": "user", "content": prompt}],
            temperature=temperature,
            max_tokens=max_tokens
        )
        return resp.choices[0].message.content.strip()
    except Exception as e:
        print(f"âš ï¸ LLM error: {e}")
        return ""

# ============================================================
# SEARCH FUNCTIONS
# ============================================================

def embed_query(query: str) -> np.ndarray:
    vec = EMBEDDER_V2.encode([f"query: {query}"], normalize_embeddings=True)
    return vec.astype("float32")

def search_chunks_direct(query: str, top_k: int = 5) -> List[int]:
    """Retrieve chunks directly using string embedding"""
    q_vec = embed_query(query)
    D, I = CHUNK_INDEX_V2.search(q_vec, top_k)
    return [int(idx) for idx in I[0] if idx >= 0 and idx < len(CHUNKS_V2)]

def search_entities(query: str, top_k: int = 15) -> List[Tuple[str, float]]:
    """Retrieve entities directly using string embedding"""
    if ENTITY_INDEX_V2 is None: return []
    q_vec = embed_query(query)
    D, I = ENTITY_INDEX_V2.search(q_vec, top_k)
    return [(ENTITY_NAMES_V2[idx], float(D[0][j])) for j, idx in enumerate(I[0]) if 0 <= idx < len(ENTITY_NAMES_V2)]

def get_entity_relationships(entity_name: str) -> List[Dict]:
    """Get all outgoing and incoming relationships for an entity"""
    rels = []
    for _, tgt, data in KG_V2.out_edges(entity_name, data=True):
        rels.append({"src": entity_name, "tgt": tgt, **data})
    for src, _, data in KG_V2.in_edges(entity_name, data=True):
        rels.append({"src": src, "tgt": entity_name, **data})
    return rels

def hybrid_query_engine(question: str, top_k_entities: int = 5, top_k_chunks: int = 5) -> Tuple[str, Dict]:
    """
    HYBRID SEARCH PIPELINE:
    1. Láº¥y top K entities tá»« query.
    2. Tá»« cÃ¡c entities Ä‘Ã³, trÃ­ch xuáº¥t táº¥t cáº£ cÃ¡c chunks liÃªn quan (source_chunks cá»§a entity).
    3. Láº¥y thÃªm top M chunks trá»±c tiáº¿p tá»« query.
    4. Trá»™n chung cÃ¡c chunks tÃ¬m Ä‘Æ°á»£c.
    5. TrÃ­ch xuáº¥t táº¥t cáº£ cÃ¡c má»‘i quan há»‡ liÃªn quan Ä‘áº¿n top K entities.
    6. ÄÆ°a táº¥t cáº£ vÃ o LLM Ä‘á»ƒ answer.
    """
    debug_info = {}
    
    # 1. Direct Chunk Search (Semantic Search)
    direct_chunks = search_chunks_direct(question, top_k=5)
    
    # 2. Extract Entities from Top Chunks for KG support AFTER topK
    chunk_entities = set()
    for c_idx in direct_chunks:
        for ent, chunks in ENTITY_TO_CHUNKS.items():
            if c_idx in chunks:
                chunk_entities.add(ent)
                
    # 3. (Optional) Entity-VDB Expand
    entity_results = search_entities(question, top_k=3)
    matched_entities_from_vdb = [e for e, score in entity_results]
    
    # Láº¥y thÃªm tá»‘i Ä‘a 2 chunks tá»« Entity-VDB (náº¿u chÆ°a cÃ³ trong direct_chunks)
    entity_linked_chunks = []
    for ent in matched_entities_from_vdb:
        for c_idx in ENTITY_TO_CHUNKS.get(ent, []):
            if c_idx < len(CHUNKS_V2) and c_idx not in direct_chunks and c_idx not in entity_linked_chunks:
                entity_linked_chunks.append(c_idx)
    entity_linked_chunks = entity_linked_chunks[:2]
    
    # Merge Entities (tá»« chunk + tá»« VDB) Ä‘á»ƒ láº¥y Relationships
    combined_entities = list(set(list(chunk_entities) + matched_entities_from_vdb))
    
    # 4. Merge Chunks (Total max 7: 5 direct + 2 entity)
    final_chunk_indices = direct_chunks + entity_linked_chunks
    
    # 5. Extract Relationships cho ALL combined entities
    rels_context = ""
    seen_rels = set()
    for ent in combined_entities:
        for rel in get_entity_relationships(ent):
            rel_key = (rel["src"], rel["tgt"])
            if rel_key not in seen_rels:
                seen_rels.add(rel_key)
                rel_name = rel.get("relation", "liÃªn_quan")
                desc = f" ({rel['description']})" if rel.get("description") else ""
                rels_context += f"â€¢ {rel['src']} --[{rel_name}]--> {rel['tgt']}{desc}\n"

    
    # Generate Context String
    ent_context_str = "\n".join([f"â€¢ {e}" for e in matched_entities_from_vdb])
    chunk_context_str = "\n".join([f"[Chunk {i+1}]: {CHUNKS_V2[idx]}" for i, idx in enumerate(final_chunk_indices)])
    
    context_str = f"""--- KHÃI NIá»†M & THá»°C THá»‚ Tá»ª CÃ‚U Há»I ---
{ent_context_str}

--- CÃC Má»I QUAN Há»† TRONG Äá»’ THá»Š (Há»– TRá»¢ Tá»ª CHUNKS VÃ€ CÃ‚U Há»I) ---
{rels_context}

--- CÃC TRÃCH ÄOáº N VÄ‚N Báº¢N (CHUNKS) ---
{chunk_context_str}"""

    prompt = f"""Báº¡n lÃ  trá»£ lÃ½ phÃ¡p lÃ½ vÃ  chuyÃªn gia vá» vÄƒn báº£n phÃ¡p luáº­t, thÃ´ng tÆ° cá»§a Viá»‡t Nam.

STRICT RULES:
1) CHá»ˆ Sá»¬ Dá»¤NG THÃ”NG TIN CÃ“ TRONG CONTEXT BÃŠN DÆ¯á»šI. KHÃ”NG sá»­ dá»¥ng kiáº¿n thá»©c bÃªn ngoÃ i hay tá»± Ä‘Æ°a ra giáº£ Ä‘á»‹nh.
2) Báº N CÃ“ THá»‚ Ã¡p dá»¥ng suy luáº­n logic vÃ  phÃ¡p lÃ½ dá»±a trÃªn cÃ¡c quy Ä‘á»‹nh trong CONTEXT (vÃ­ dá»¥: tÃ­nh toÃ¡n sá»‘ tiá»n, Ã¡p dá»¥ng quy táº¯c vÃ o tÃ¬nh huá»‘ng).
3) Náº¿u CONTEXT KHÃ”NG CHá»¨A Ä‘á»§ quy Ä‘á»‹nh Ä‘á»ƒ tráº£ lá»i há»£p lÃ½, Báº®T BUá»˜C TRáº¢ Lá»œI NGAY Má»˜T CÃ‚U DUY NHáº¤T:
   KhÃ´ng tÃ¬m tháº¥y thÃ´ng tin liÃªn quan
4) Æ¯u tiÃªn láº¥y cÃ¡c Ä‘iá»u khoáº£n, thÃ´ng tÆ° phÃ¹ há»£p nháº¥t.
5) Náº¿u nhiá»u pháº§n trong CONTEXT chá»©a thÃ´ng tin mÃ¢u thuáº«n, hÃ£y chá»‰ ra sá»± mÃ¢u thuáº«n.

REASONING RULE (QUAN TRá»ŒNG):
- CÃ¢u há»i cÃ³ thá»ƒ mÃ´ táº£ má»™t tÃ¬nh huá»‘ng thá»±c táº¿ (case study).
- CÃ¡c Ä‘á»‹nh má»©c chi phÃ­ hoáº·c thuáº¿ cáº§n Ã¡p dá»¥ng chÃ­nh xÃ¡c theo báº£ng luáº­t trong CONTEXT.
- Báº¡n pháº£i tuÃ¢n thá»§:
  (a) XÃ¡c Ä‘á»‹nh quy táº¯c / má»©c phÃ­ liÃªn quan trong CONTEXT.
  (b) Ãp dá»¥ng tá»· lá»‡ / sá»‘ tiá»n Ä‘Ã³ vÃ o dá»¯ kiá»‡n cá»§a tÃ¬nh huá»‘ng thá»±c táº¿. Tiá»ƒn hÃ nh cá»™ng trá»« nhÃ¢n chia rÃµ rÃ ng.
  (c) ÄÆ°a ra Káº¾T LUáº¬N Cá»¤ THá»‚ (cÃ³ sá»‘ liá»‡u náº¿u cáº§n) Ä‘Æ°á»£c suy ra logic tá»« CONTEXT.

OUTPUT FORMAT (Báº¯t buá»™c pháº£i cÃ³ Ä‘Ãºng cÃ¡c Headline nÃ y):

Answer:
- TrÃ¬nh bÃ y suy luáº­n vÃ  káº¿t luáº­n ngáº¯n gá»n, chi tiáº¿t cÃ¡c bÆ°á»›c tÃ­nh toÃ¡n náº¿u cÃ³ (Quy táº¯c â†’ Ãp dá»¥ng â†’ Káº¿t luáº­n). TRáº¢ Lá»œI Báº°NG TIáº¾NG VIá»†T.

CÆ¡ sá»Ÿ phÃ¡p lÃ½:
- <NÃªu rÃµ Khoáº£n/Äiá»u/ThÃ´ng tÆ° nÃ o trong CONTEXT Ä‘Ã£ dÃ¹ng Ä‘á»ƒ tráº£ lá»i>

TrÃ­ch dáº«n:
- "<TrÃ­ch dáº«n chÃ­nh xÃ¡c tá»« CONTEXT Ä‘á»ƒ lÃ m báº±ng chá»©ng (tá»‘i Ä‘a ~30 chá»¯/trÃ­ch dáº«n)>"

CONTEXT:
{context_str}

CÃ‚U Há»I Tá»”NG Há»¢P: {question}

TRáº¢ Lá»œI:"""

    answer = call_llm_query(prompt)
    
    debug_info["context_recall"] = context_str
    debug_info["num_entities"] = len(combined_entities)
    debug_info["num_chunks"] = len(final_chunk_indices)
    debug_info["num_relationships"] = len(seen_rels)
    
    return answer, debug_info

# ============================================================
# EVALUATION BATCH SCRIPT
# ============================================================
if __name__ == "__main__":
    import sys
    
    EVAL_FILE = "eval-thue.json"
    OUTPUT_FILE = "eval-thue-output.json"
    
    if not os.path.exists(EVAL_FILE):
        print(f"File {EVAL_FILE} not found!")
        sys.exit(1)
        
    with open(EVAL_FILE, "r", encoding="utf-8") as f:
        eval_data = json.load(f)
        
    print(f"ğŸš€ Báº¯t Ä‘áº§u Ä‘Ã¡nh giÃ¡ cho {len(eval_data)} cÃ¢u há»i...")
    
    results = []
    for i, item in enumerate(tqdm(eval_data)):
        q_text = item.get("query", "")
        case_text = item.get("case", "")
        if case_text:
            q = f"[TÃ¬nh huá»‘ng thá»±c táº¿]: {case_text}\n[CÃ¢u há»i]: {q_text}"
        else:
            q = q_text
            
        # TÄƒng thá»i gian chá» lÃªn 4.5 giÃ¢y Ä‘á»ƒ trÃ¡nh lá»—i Rate Limit (429) do Ä‘Ã¡nh giÃ¡ 50 cÃ¢u liÃªn tá»¥c
        time.sleep(4.5) 
        
        my_answer, debug = hybrid_query_engine(q)
        
        res = {
            "id": i + 1,
            "type": item.get("type", ""),
            "query": q_text,
            "case": case_text,
            "expected_answer": item.get("expected_answer", ""),
            "my_answer": my_answer,
            "debug_info": {
                "num_entities": debug["num_entities"],
                "num_chunks": debug["num_chunks"],
                "num_relationships": debug["num_relationships"],
                "context_recall": debug["context_recall"]
            }
        }
        results.append(res)
        
        # Save tiáº¿n trÃ¬nh liÃªn tá»¥c Ä‘á»ƒ phÃ²ng há» bá»‹ ngáº¯t
        with open(OUTPUT_FILE, "w", encoding="utf-8") as out_f:
            json.dump(results, out_f, ensure_ascii=False, indent=2)
            
    print(f"\nâœ… ÄÃ£ hoÃ n thÃ nh! Káº¿t quáº£ Ä‘Æ°á»£c lÆ°u táº¡i {OUTPUT_FILE}")
